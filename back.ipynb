{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Categoría', 'Enlace', 'Nombre', 'ImagenURL', 'Calorías',\n",
      "       'Ingredientes', 'Nutrientes', 'Vitaminas y Minerales'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datos.csv', delimiter=';')\n",
    "\n",
    "#print(df.head())\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convertir la columna 'Ingredientes' en listas y aplicar One-Hot Encoding\n",
    "df['Ingredientes'] = df['Ingredientes'].apply(lambda x: x.split(','))  # Convertir en listas\n",
    "mlb = MultiLabelBinarizer()\n",
    "ingredientes_encoded = mlb.fit_transform(df['Ingredientes'])\n",
    "\n",
    "# Crear un DataFrame para los ingredientes codificados\n",
    "ingredientes_df = pd.DataFrame(ingredientes_encoded, columns=mlb.classes_)\n",
    "df = pd.concat([df, ingredientes_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['Calorías'] = scaler.fit_transform(df[['Calorías']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Categoría'] = df['Categoría'].astype('category')\n",
    "df['Nutrientes'] = df['Nutrientes'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Baja_en_grasa'] = df['Nutrientes'].apply(lambda x: 1 if 'baja en grasa' in x else 0)\n",
    "df['Alto_en_proteinas'] = df['Nutrientes'].apply(lambda x: 1 if 'alto en proteínas' in x else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Es_desayuno'] = df['Categoría'].apply(lambda x: 1 if 'desayuno' in x.lower() else 0)\n",
    "df['Es_almuerzo'] = df['Categoría'].apply(lambda x: 1 if 'almuerzo' in x.lower() else 0)\n",
    "df['Es_cena'] = df['Categoría'].apply(lambda x: 1 if 'cena' in x.lower() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convertir los ingredientes en vectores numéricos\n",
    "vectorizer = TfidfVectorizer()\n",
    "ingredientes_vectores = vectorizer.fit_transform(df['Ingredientes'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Normalizar las calorías y unir con los vectores de ingredientes\n",
    "X = pd.concat([pd.DataFrame(ingredientes_vectores.toarray()), df[['Calorías']]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definir las etiquetas de salida\n",
    "y = df[['Baja_en_grasa', 'Alto_en_proteinas', 'Es_desayuno', 'Es_almuerzo', 'Es_cena']]\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 17:41:19.660232: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-24 17:41:19.660793: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-24 17:41:19.663686: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-24 17:41:19.672707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740433279.687347   29740 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740433279.691353   29740 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-24 17:41:19.707494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edfermachado/Descargas/Samsung/proyecto/ProyectoFinalSIC/miau/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-02-24 17:41:21.559013: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.3164 - loss: 0.6581 - val_accuracy: 0.0125 - val_loss: 0.4979\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0746 - loss: 0.4110 - val_accuracy: 0.0750 - val_loss: 0.2032\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1023 - loss: 0.1885 - val_accuracy: 0.0750 - val_loss: 0.1711\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1073 - loss: 0.1533 - val_accuracy: 0.0750 - val_loss: 0.1623\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1300 - loss: 0.1490 - val_accuracy: 0.0750 - val_loss: 0.1552\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1234 - loss: 0.1483 - val_accuracy: 0.0750 - val_loss: 0.1524\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1144 - loss: 0.1423 - val_accuracy: 0.0750 - val_loss: 0.1496\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1309 - loss: 0.1288 - val_accuracy: 0.0875 - val_loss: 0.1486\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1402 - loss: 0.1225 - val_accuracy: 0.0938 - val_loss: 0.1461\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1388 - loss: 0.1116 - val_accuracy: 0.1000 - val_loss: 0.1466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f128444fa10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de entrada (tamaño igual a las características de entrada)\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Capa oculta\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Capa de salida con activación sigmoide para clasificación binaria\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))  # Una salida para cada etiqueta\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/edfermachado/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Obtener la ruta del directorio actual\n",
    "directorio_actual = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Descargar las stopwords de NLTK en el directorio actual\n",
    "nltk.download('stopwords', download_dir=\"\")\n",
    "\n",
    "# Cargar el modelo de español de spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    # Descargar el modelo si no está instalado\n",
    "    spacy.cli.download(\"es_core_news_sm\")\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Obtener las stopwords en español\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "def preprocesar_pregunta(pregunta):\n",
    "    # Convertir la pregunta a minúsculas\n",
    "    pregunta = pregunta.lower()\n",
    "\n",
    "    # Eliminar caracteres especiales, números y puntuación\n",
    "    pregunta = re.sub(r'[^a-záéíóúüñ\\s]', '', pregunta)\n",
    "\n",
    "    # Tokenizar la pregunta usando spaCy\n",
    "    doc = nlp(pregunta)\n",
    "\n",
    "    # Eliminar stopwords y aplicar lematización\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in stop_words]\n",
    "\n",
    "    # Unir los tokens preprocesados de vuelta en una cadena\n",
    "    pregunta_normalizada = \" \".join(tokens)\n",
    "\n",
    "    return pregunta_normalizada\n",
    "\n",
    "# Ejemplo de uso\n",
    "pregunta_ejemplo = \"¿Cuáles son los mejores lugares para visitar en Madrid?\"\n",
    "pregunta_preprocesada = preprocesar_pregunta(pregunta_ejemplo)\n",
    "print(pregunta_preprocesada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 800, but received input with shape (1, 799)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 799), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Ejemplo de uso\u001b[39;00m\n\u001b[1;32m     19\u001b[0m pregunta \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceta de desayuno con pollo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m resultados \u001b[38;5;241m=\u001b[39m \u001b[43mresponder_a_consulta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocesar_pregunta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpregunta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Mostrar los resultados\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resultados, \u001b[38;5;28mlist\u001b[39m):\n",
      "Cell \u001b[0;32mIn[29], line 9\u001b[0m, in \u001b[0;36mresponder_a_consulta\u001b[0;34m(pregunta)\u001b[0m\n\u001b[1;32m      6\u001b[0m pregunta_vectorizada \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([pregunta_normalizada])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Predecir las etiquetas (ingredientes, características nutricionales, etc.)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m predicciones \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpregunta_vectorizada\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Mapear las predicciones a las recetas relevantes\u001b[39;00m\n\u001b[1;32m     12\u001b[0m recetas \u001b[38;5;241m=\u001b[39m df[predicciones \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m]  \u001b[38;5;66;03m# Ajusta el umbral según lo que necesites\u001b[39;00m\n",
      "File \u001b[0;32m~/Descargas/Samsung/proyecto/ProyectoFinalSIC/miau/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Descargas/Samsung/proyecto/ProyectoFinalSIC/miau/lib/python3.11/site-packages/keras/src/layers/input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    224\u001b[0m             value,\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         }:\n\u001b[0;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 800, but received input with shape (1, 799)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 799), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "def responder_a_consulta(pregunta):\n",
    "    # Preprocesar la pregunta del usuario (convertir a minúsculas, tokenizar, etc.)\n",
    "    pregunta_normalizada = preprocesar_pregunta(pregunta)\n",
    "\n",
    "    # Convertir la pregunta en un vector utilizando TF-IDF\n",
    "    pregunta_vectorizada = vectorizer.transform([pregunta_normalizada])\n",
    "\n",
    "    # Predecir las etiquetas (ingredientes, características nutricionales, etc.)\n",
    "    predicciones = model.predict(pregunta_vectorizada)\n",
    "\n",
    "    # Mapear las predicciones a las recetas relevantes\n",
    "    recetas = df[predicciones > 0.5]  # Ajusta el umbral según lo que necesites\n",
    "    return recetas[['Nombre', 'Ingredientes']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "pregunta = \"Receta de desayuno con pollo\"\n",
    "resultados = responder_a_consulta(preprocesar_pregunta(pregunta))\n",
    "\n",
    "# Mostrar los resultados\n",
    "if isinstance(resultados, list):\n",
    "    for receta in resultados:\n",
    "        print(f\"Receta: {receta['Nombre']}, Categoría: {receta['Categoría']}, Ingredientes: {receta['Ingredientes']}\")\n",
    "else:\n",
    "    print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only join an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Entrenar el vectorizador con los ingredientes del DataFrame completo\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m ingredientes_vectores \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIngredientes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Comprobar el número de características del vectorizador\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNúmero de características en el vectorizador: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mingredientes_vectores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Descargas/Samsung/proyecto/ProyectoFinalSIC/miau/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Descargas/Samsung/proyecto/ProyectoFinalSIC/miau/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Descargas/Samsung/proyecto/ProyectoFinalSIC/miau/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Descargas/Samsung/proyecto/ProyectoFinalSIC/miau/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Descargas/Samsung/proyecto/ProyectoFinalSIC/miau/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[32], line 18\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Entrenar el vectorizador con los ingredientes del DataFrame completo\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m ingredientes_vectores \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIngredientes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x)))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Comprobar el número de características del vectorizador\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNúmero de características en el vectorizador: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mingredientes_vectores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only join an iterable"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Asegurémonos de que el vectorizador esté entrenado con todos los datos de ingredientes\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Entrenar el vectorizador con los ingredientes del DataFrame completo\n",
    "ingredientes_vectores = vectorizer.fit_transform(df['Ingredientes'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Comprobar el número de características del vectorizador\n",
    "print(f\"Número de características en el vectorizador: {ingredientes_vectores.shape[1]}\")\n",
    "\n",
    "# Luego, asegurarnos de que en la predicción se mantenga el mismo número de características\n",
    "def preprocesar_pregunta(pregunta):\n",
    "    # Convertir la pregunta a minúsculas\n",
    "    pregunta = pregunta.lower()\n",
    "\n",
    "    # Eliminar caracteres especiales, números y puntuación\n",
    "    pregunta = re.sub(r'[^a-záéíóúüñ\\s]', '', pregunta)\n",
    "\n",
    "    # Tokenizar la pregunta usando spaCy\n",
    "    doc = nlp(pregunta)\n",
    "\n",
    "    # Eliminar stopwords y aplicar lematización\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in stop_words]\n",
    "\n",
    "    # Unir los tokens preprocesados de vuelta en una cadena\n",
    "    pregunta_normalizada = \" \".join(tokens)\n",
    "\n",
    "    return pregunta_normalizada\n",
    "\n",
    "def responder_a_consulta(pregunta):\n",
    "    # Preprocesar la pregunta\n",
    "    pregunta_normalizada = preprocesar_pregunta(pregunta)\n",
    "\n",
    "    # Convertir la pregunta en un vector utilizando el mismo TF-IDF que entrenamos\n",
    "    pregunta_vectorizada = vectorizer.transform([pregunta_normalizada])\n",
    "\n",
    "    # Verificar que el número de características de la entrada coincide con el modelo\n",
    "    if pregunta_vectorizada.shape[1] != X_train.shape[1]:\n",
    "        raise ValueError(f\"El número de características es incompatible. Esperado {X_train.shape[1]}, pero se recibió {pregunta_vectorizada.shape[1]}.\")\n",
    "\n",
    "    # Predecir las probabilidades para cada etiqueta\n",
    "    predicciones = model.predict(pregunta_vectorizada)\n",
    "\n",
    "    # Mapear las predicciones\n",
    "    etiquetas_predichas = (predicciones > 0.5).astype(int)  # Ajuste de umbral a 0.5 para todas las etiquetas\n",
    "\n",
    "    # Filtrar las recetas que cumplen con las condiciones\n",
    "    recetas_filtradas = df.copy()\n",
    "\n",
    "    # Aplicar las etiquetas predichas\n",
    "    if etiquetas_predichas[0][0] == 1:  # Baja_en_grasa\n",
    "        recetas_filtradas = recetas_filtradas[recetas_filtradas['Baja_en_grasa'] == 1]\n",
    "    if etiquetas_predichas[0][1] == 1:  # Alto_en_proteinas\n",
    "        recetas_filtradas = recetas_filtradas[recetas_filtradas['Alto_en_proteinas'] == 1]\n",
    "    if etiquetas_predichas[0][2] == 1:  # Es_desayuno\n",
    "        recetas_filtradas = recetas_filtradas[recetas_filtradas['Es_desayuno'] == 1]\n",
    "    if etiquetas_predichas[0][3] == 1:  # Es_almuerzo\n",
    "        recetas_filtradas = recetas_filtradas[recetas_filtradas['Es_almuerzo'] == 1]\n",
    "    if etiquetas_predichas[0][4] == 1:  # Es_cena\n",
    "        recetas_filtradas = recetas_filtradas[recetas_filtradas['Es_cena'] == 1]\n",
    "\n",
    "    # Mostrar resultados\n",
    "    if not recetas_filtradas.empty:\n",
    "        return recetas_filtradas[['Nombre', 'Ingredientes']]\n",
    "    else:\n",
    "        return \"No se encontraron recetas que coincidan con los parámetros.\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "pregunta = \"Receta de desayuno con pollo\"\n",
    "resultados = responder_a_consulta(pregunta)\n",
    "\n",
    "# Mostrar los resultados\n",
    "if isinstance(resultados, pd.DataFrame):\n",
    "    for _, receta in resultados.iterrows():\n",
    "        print(f\"Receta: {receta['Nombre']}, Ingredientes: {receta['Ingredientes']}\")\n",
    "else:\n",
    "    print(resultados)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
